# [Integration Name] Learning Tests

> Created: [DATE] | System: [SDK/API name + version]
> Documentation: [link to official docs]
> Status: Not run / Partial / Complete

---

## Key Findings

*Update this section AFTER running tests, not before.*

1. ✅/❌/⚠️ [Assumption]: [Actual behavior observed]
2. ✅/❌/⚠️ [Assumption]: [Actual behavior observed]

---

## Assumptions to Verify

1. [What we think happens when X]
2. [Expected response shape for Y]
3. [Edge case: what happens on timeout/error/invalid input]
4. [Rate limits, pagination, encoding quirks]

---

## Test: [Assumption 1 Name]

```[language]
// What we're testing: [description]
// Expected: [what we think will happen]
// Actual: [UPDATE AFTER RUNNING]
```

---

## Test: [Assumption 2 Name]

```[language]
// What we're testing: [description]
// Expected: [what we think will happen]
// Actual: [UPDATE AFTER RUNNING]
```

---

## Contract Assertions

Re-run these when:

- [ ] SDK/API version bumps
- [ ] "Something changed" but unclear what
- [ ] Starting new phase of integration work
- [ ] External provider announces API changes
- [ ] Unexpected behavior in production

---

*Store in: `tests/contracts/[name]/` or `docs/[name]/learning-tests/`*
*These are NOT CI tests — run manually when verifying external contracts.*
